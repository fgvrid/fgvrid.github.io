<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>FGVRID Workshop@ICPR 2020</title>
<meta name="description" content="">
<meta name="theme-color" content="#a2251c">
<meta name="author" content="Jesse Chou">
<link rel="shortcut icon" href="img/favicon.png">
	
<script src="js/jquery.min.js"></script>
<script src="js/jquery.knob.js"></script>
<script src="js/jquery.throttle.js"></script>
<script src="js/jquery.classycountdown.js"></script>

<link rel="stylesheet" type="text/css" href="css/style.css" />
</head>
<body>
	
<div class="banner">
  <div class="banner-inner">
    <div class="header">
      <div class="container">
        <div class="flex-m flex-space-between-m"><a class="logo" href="/"><span class="sr">Workshop</span></a>
          <div class="navigation right-m">
            <ul class="menu">
              <li class="menu-item"><a href="#news">Latest News</a></li>
			  <li class="menu-item"><a href="#call">Call for papers</a></li>
              <li class="menu-item"><a href="#speakers">Keynote speakers</a></li>
              <li class="menu-item"><a href="#papers">Accepted papers</a></li>
              <li class="menu-item"><a href="#program">Program</a></li>
              <li class="menu-item"><a href="#people">People involved</a></li>
			  <li class="menu-item"><a href="#contact">Contact</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
	<div class="container pad-top-30 pad-top-40-m">
      <div class="pad-top-50-m pad-bottom-50-m">
        <h1 class="banner-heading"><span class="banner-heading-text-1"> FGVRID Workshop: Fine-Grained </span><span class="banner-heading-text-2">Visual <br>Recognition </span><span class="banner-heading-text-3">and re-IDentification </span></h1>
		<div class="flex-m flex-center-m" align="left">
          <p class="fs-4 no-margin pad-top-10 pad-bottom-10 col-11-m col-4-l banner-text-1">Submission deadline: Oct <strike>10th</strike> 19th, 2020<br>Author notification: Nov 10th, 2020<br>Camera-ready submission: Nov 15th, 2020<br>Finalized program: Dec 1st, 2020</p>
        </div>
      </div>
</div>
	<div class="container">
    <div class="flex-m flex-center-m">
      <div class="col-11-m col-9-l">
		<div class="textwidget" align="left">
		  <p><font color="#a2251c"> <strong>FGVRID@ICPR 2020 begins:11 January 2021</strong></font></p>
		</div>
		<div id="countdown2" class="ClassyCountdownDemo" align="center"></div>
		  <script>$('#countdown2').ClassyCountdown({
		end: Date.UTC(2021, 00, 11, 00, 00, 00) / 1000,
		now: $.now() / 1000,
		labels: true,
		style: {
			element: "",
			textResponsive: .4,
			days: {
				gauge: {
					thickness: .01,
					bgColor: "rgba(0,0,0,0.05)",
					fgColor: "#1abc9c"
				},
				textCSS: 'font-family:\'Open Sans\'; font-size:25px; font-weight:300; color:#34495e;'
			},
			hours: {
				gauge: {
					thickness: .01,
					bgColor: "rgba(0,0,0,0.05)",
					fgColor: "#2980b9"
				},
				textCSS: 'font-family:\'Open Sans\'; font-size:25px; font-weight:300; color:#34495e;'
			},
			minutes: {
				gauge: {
					thickness: .01,
					bgColor: "rgba(0,0,0,0.05)",
					fgColor: "#8e44ad"
				},
				textCSS: 'font-family:\'Open Sans\'; font-size:25px; font-weight:300; color:#34495e;'
			},
			seconds: {
				gauge: {
					thickness: .01,
					bgColor: "rgba(0,0,0,0.05)",
					fgColor: "#f39c12"
				},
				textCSS: 'font-family:\'Open Sans\'; font-size:25px; font-weight:300; color:#34495e;'
			}

		},
		onEndCallback: function() {
			console.log("Time out!");
		}
	});</script>
		</div>
	</div>
	  </div>
  </div>
</div>

<div class="news-block" id="news">
  <div class="container">
    <div class="flex-m flex-center-m">
      <div class="col-11-m col-9-l">
		  <h2 class="heading pad-bottom-15"> <span class="big-heading">Latest News</span></h2>
		  <p style="text-align:justify; text-justify:inter-ideograph;">
		  <ul>
  
<li>2020/07/01: We appreciate that Professor Alberto Del Bimbo have confirmed his keynote speech.</li>
<li>2020/05/29: A <a href="https://dl.acm.org/journal/tomm/special-issues" target="_blank">special issue</a> call: <a href="https://dl.acm.org/pb-assets/static_journal_pages/tomm/pdf/CFP_FGVRreID-1592406610240.pdf" target="_blank">"Fine-Grained Visual Recognition and re-Identification"</a> is going to be organized in ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM). Submission deadline: January 15, 2021.</li>
<li>2020/04/17: We sincerely looking forward to paper submissions, <a href="https://cmt3.research.microsoft.com/FGVRID2020" target="_blank"><strong>HERE</strong></a>.</li>
<li>2020/03/27: We appreciate that Rita Cucchiara, Weishi Zheng and Elisa Ricci have confirmed their keynote speeches.</li>	
</ul>
		</p>
      </div>
    </div>
  </div>
</div>

<div class="container" id="call">
    <div class="flex-m flex-center-m">
      <div class="col-11-m col-9-l">
        <h2 class="heading pad-bottom-15"> <span class="big-heading">Call for papers</span></h2>
	      <p style="text-align:justify; text-justify:inter-ideograph;"><strong>Templates:</strong>
         The publication is handled by springer, so the springer template is used. Please directly refer to the following link (always up to date version) for word/latex templates:
		<a href="https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines" target="_blank">https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines</a></p>
        <p style="text-align:justify; text-justify:inter-ideograph;"><strong>Paper types:</strong>
        Since Springer template is used, the page limit is different to the main conference: For full paper, the number of pages is limited between 12-15. For short paper, the number of pages is limited between 6-8.</p>
        <p style="text-align:justify; text-justify:inter-ideograph;"><strong>Submission system:</strong>
        <a href="https://cmt3.research.microsoft.com/FGVRID2020" target="_blank"><strong>https://cmt3.research.microsoft.com/FGVRID2020</strong></a></p>
      <p style="text-align:justify; text-justify:inter-ideograph;"><strong>Camera-ready Guideline:</strong></p>
	    <p style="text-align:justify; text-justify:inter-ideograph;">Information and tasks for authors:
		<ul>
		  <li>We need all source files (LaTeX files with all the associated style files, special fonts and eps files, or Word or rtf files) and the final pdfs of all of the papers. For papers prepared using LaTeX, authors should supply the underlying bib file for the references. Springer typesetters will use this to create the bbl file. Please note that we cannot include hyperlinks in references.</li>
	  	  <li>A mixture of LaTeX and Word files is fine. Please do not send any older versions of papers. There should be one set of source files and one pdf file per paper. Springer type-setters require the author-created pdfs in order to check the proper representation of symbols, figures, etc.</li>
		  <li>When submitting their paper, the authors should allot a corresponding author, who must be available to carry out a proof check of the paper. He or she is given a 72-hour time-slot to do so. The corresponding author should be clearly marked as such in the header of the paper. He or she is also the one who signs the copyright form on behalf of all of the authors. Please note that the corresponding author cannot be changed once the paper and the copyright form have been sent to Springer.</li>
		  <li>We encourage the inclusion of all of the authorsâ€™ email addresses and ORCIDs in the header, but at the very least, the email address of the corresponding author should be present.</li>
	      <li>The corresponding author should compile, sign and send the Consent to Publish form 
		      <a href="https://fgvrid.github.io/ICPR2020W-Contract_Book_Contributor_Consent_to_Publish_LNCS_SIP.pdf" target="_blank"><strong><font color="red">[ICPR2020W-Contract_Book_Contributor_Consent_to_Publish_LNCS_SIP.pdf]</a></font></strong> together with the paper. We do not accept digital signatures on the copyright right forms at present. If you have any queries regarding copyright, please contact Springer well in advance of publication. Each form should be saved in the individual folder containing all of the files pertaining to a particular paper.</li>
together with the paper. We do not accept digital signatures on the copyright right forms at present. If you have any queries regarding copyright, please contact Springer well in advance of publication. Each form should be saved in the individual folder containing all of the files pertaining to a particular paper.</li>
		</ul></p>
	<p style="text-align:justify; text-justify:inter-ideograph;">Please submit the camera-ready and consent to Publish form before <strong><font color="red">November 18th, 2020 (11:59PM Pacific Time) </font></strong>via easychair system. All files should be zipped into a single zip/rar package when uploading to the system.</p>
<p style="text-align:justify; text-justify:inter-ideograph;">--------------------------------</p>
		<p style="text-align:justify; text-justify:inter-ideograph;">
			 The full-day workshop on Fine-Grained Visual Recognition and re-IDentification (FGVRID 2020) will be organized on January 11, 2021, in conjunction with ICPR 2020, 
			 <a href="https://www.icpr2020.it" target="_blank">The 25th International Conference on Pattern Recognition</a>, Milan, Italy, January 10-15, 2021.</p>
		<p style="text-align:justify; text-justify:inter-ideograph;">This workshop will bring together researchers from subfields of patter recognition that have seen growing activity in the past few years: fine-grained visual recognition and person/vehicle re-identification.</p>
        <p style="text-align:justify; text-justify:inter-ideograph;">The ubiquitous surveillance cameras are generating huge amount of videos. Automatic video content analysis and recognition are thus desirable for effective utilization of those data. Fine-Grained Visual Recognition and Re-Identification (FGVRID) aims to accurately identify visual objects and match re-appearing targets, e.g., persons and vehicles from a large set of images and videos. It has the potential to offer an unprecedented possibility for intelligent video processing and analysis, as well as to explore the promising applications on public security. </p>
        <p style="text-align:justify; text-justify:inter-ideograph;">The FGVRID workshop wishes to bring together researchers from fine-grained visual categorization, as well as person/ vehicle ReID communities, and to foster discussions and exchange of ideas between them. FGVRID is not a traditional search or classification task due to its goal of accurately identifying visual objects. First, proper detection algorithms should be designed to locate objects and their parts in videos before proceeding to the identification step. Second, the visual appearance of an object is easily affected by many factors like viewpoint changes and camera parameter differences, etc. Third, annotating the fine-grained identity or category cues is expensive and time consuming. Finally, to cope with the large-scale data, scalable indexing or feature coding algorithms should be designed to ensure the online recognition efficiency. Aiming to seek novel solutions and possibilities in FGVRID, this workshop will have in-depth discussions on those issues and aims to go beyond toy datasets and small-scale algorithms. Specifically, the covered topics include, but are not limited to:
		<ul>
		  <li>Unsupervised, semi-supervised, and transfer learning algorithms</li>
	  	  <li>Robust object detection and tracking in the wild</li>
		  <li>Efficient and effective video representations</li>
		  <li>Object parsing and layout estimation</li>
	      <li>Large-scale indexing, feature coding, and retrieval algorithms</li>
		  <li>Fine-grained visual classification</li>
		  <li>New problems and datasets for fine-grained visual recognition and re-identification</li>
		</ul></p>
		
	</div>
    </div>
</div>
	
<div class="container" id="speakers">
    <div class="flex-m flex-center-m">
      <div class="col-11-m col-9-l">
        <h2 class="heading pad-bottom-15"><span class="big-heading">Keynote speakers</span></h2>
		<table>
		<tr>
			<td>
                        <div class="organizer-avatar"><img src="img/Alberto Del Bimbo.png" alt="Alberto Del Bimbo">
            	        <p>Alberto Del Bimbo<br>University of Florence<br>alberto.delbimbo@unifi.it</p></div>
                        </td>
			<td>
			<div class="organizer-avatar"><img src="img/Rita Cucchiara.png" alt="Rita Cucchiara">
            	        <p>Rita Cucchiara<br>University of Modena and Reggio Emilia<br>rita.cucchiara@unimore.it</p></div>
			</td>
		<tr>
		</tr>
			<td>
                        <div class="organizer-avatar"><img src="img/Wei-Shi Zheng.png" alt="Weishi Zheng">
			<p>Weishi Zheng<br>Sun Yat-sen University<br>zhwshi@mail.sysu.edu.cn</p></div>                        
			</td>
			<td>
                        <div class="organizer-avatar"><img src="img/Elisa Ricci.png" alt="Elisa Ricci">
            	        <p>Elisa Ricci<br>University of Trento<br>e.ricci@unitn.it</p></div>
			</td>
		</tr>
		</table>
      </div>
    </div>
</div>
	
<div class="container" id="papers">
    <div class="flex-m flex-center-m">
      <div class="col-11-m col-9-l">
        <h2 class="heading pad-bottom-15"><span class="big-heading">Accepted papers</span></h2>
        <p class="lead" style="text-align:justify; text-justify:inter-ideograph;">TBD </p>
      </div>
    </div>
  </div>
	
<div class="container" id="program">
    <div class="flex-m flex-center-m">
      <div class="col-11-m col-9-l">
        <h2 class="heading pad-bottom-15"><span class="big-heading big-heading-secondary">Program</span></h2>
		<table border="1" cellpadding="6">
		<tr>
			<th>Time in Europe â€Ž(UTC+1)â€Ž</th>
			<th>Title</th>
			<th>Presenter</th>
		</tr>
		<tr>
			<td><p>12:00-12:05</p></td>
			<td><p>Opening</p></td>
			<td><p>Shiliang Zhang</p></td>
		</tr>
		<tr>
			<td><p>12:05-12:45</p></td>
			<td><p>Semi-Supervised Learning for Fine-Grained Classification</p></td>
			<td><p>Alberto del Bimbo</p></td>
		</tr>
		<tr>
			<td><p>12:45-13:25</p></td>
			<td><p><font color="red">To be confirmed</font></p></td>
			<td><p>Rita Cucchiara</p></td>
		</tr>
		<tr>
			<td><p>13:25-13:40</p></td>
			<td><p>Densely Annotated Photorealistic Virtual Dataset Generation for Abnormal Event Detection</p></td>
			<td><p>Rico Montulet, Alexia Briassouli</p></td>
		</tr>
		<tr>
			<td><p>13:40-13:55</p></td>
			<td><p>Unsupervised Domain Adaptive Re-Identification with Feature Adversarial Learning and Self-Similarity Clustering</p></td>
			<td><p>Tianyi Yan, Haiyun Guo, Songyan Liu, Chaoyang Zhao, Ming Tang, Jinqiao Wang</p></td>
		</tr>
		<tr>
			<td><p>13:55-14:10</p></td>
			<td><p>A Framework for Jointly Training GAN with Person Re-Identification Model</p></td>
			<td><p>Zhongwei Zhao, Qian Zhang, Ran Song, Wei Zhang</p></td>
		</tr>
		<tr>
			<td colspan="3" align="center"><b>Break(10 minutes)</b></td>
		</tr>
		<tr>
			<td><p>14:20-15:00</p></td>
			<td><p>Weakly Supervised Person Re-Identification</p></td>
			<td><p>Weishi Zheng</p></td>
		</tr>
		<tr>
			<td><p>15:00-15:40</p></td>
			<td><p>Learning to Adapt: Domain Adaptation and Generalization for Robust and Fine-Grained Recognition</p></td>
			<td><p>Elisa Ricci</p></td>
		</tr>
		<tr>
			<td><p>15:40-15:55</p></td>
			<td><p>Interpretable Attention Guided Network for Fine-grained Visual Classification</p></td>
			<td><p>Zhenhuan Huang, Xiaoyue Duan, Bo Zhao, Jinhu Lu, Baochang Zhang</p></td>
		</tr>
		<tr>
			<td><p>15:55-16:10</p></td>
			<td><p>Use of Frequency Domain for Complexity Reduction of Convolutional Neural Netoworks</p></td>
			<td><p>Kamran Chitsaz, Mohsen Hajabdollahi, Pejman Khadivi, Shadrokh Samavi, Nader Karimi, Shahram Shirani</p></td>
		</tr>
		<tr>
			<td><p>16:10-16:25</p></td>
			<td><p>From Coarse to Fine: Hierarchical Structure-Aware Video Summarization</p></td>
			<td><p>Wenxu Li, Gang Pan, Chen Wang, Jianye Hao, Zhen</p></td>
		</tr>
		<tr>
			<td><p>16:25-16:40</p></td>
			<td><p>ADNet: Temporal Anomaly Detection in Surveillance Videos</p></td>
			<td><p>Halil  Ä°brahim Ã–ztÃ¼rk, Ahmet Burak Can</p></td>
		</tr>
		</table>
	      <br>
	</div>
    </div>
  </div>

<div class="container" id="people">
    <div class="flex-l flex-center-l">
      <div class="col-11-m col-9-l">
        <h2 class="heading pad-bottom-15"><span class="big-heading big-heading-secondary">People involved</span></h2>
		 <p class="lead"><strong>Organizers</strong></p>
		  <table>
		  <tr>
			<td>
            <div class="organizer-avatar"><img src="img/Shiliang-Zhang.png" alt="Shiliang Zhang">
				<p>Shiliang Zhang<br>Peking University<br>slzhang.jdl@pku.edu.cn</p></div>
            </td>
			<td>
            <div class="organizer-avatar"><img src="img/Guorong-Li.png" alt="Guorong Li">
            	<p>Guorong Li<br>University of Chinese Academy of Sciences<br>liguorong@ucas.edu.cn</p></div>
			</td>
		  </tr>
	      <tr>
			<td>
            <div class="organizer-avatar"><img src="img/Weigang-Zhang.png" alt="Weigang Zhang">
            	<p>Weigang Zhang<br>Harbin Institute of Technology, Weihai<br>wgzhang@hit.edu.cn</p></div>
			</td>
			<td>
            <div class="organizer-avatar"><img src="img/Qingming-Huang.png" alt="Qingming Huang">
            	<p>Qingming Huang<br>University of Chinese Academy of Sciences<br>qmhuang@ucas.ac.cn</p></div>
			</td>
		  </tr>
	      <tr>
			<td>
            <div class="organizer-avatar"><img src="img/Nicu-Sebe.png" alt="Nicu Sebe">
           	 <p>Nicu Sebe<br>University of Trento<br>niculae.sebe@unitn.it</p></div>
			</td>
		 </tr>
		  </table>
      </div>
    </div>
  </div>

<div class="container" id="contact">
    <div class="flex-l flex-center-l">
      <div class="col-11-m col-9-l">
		  <h2 class="heading pad-bottom-15"><span class="big-heading big-heading-secondary">Contact Us</span></h2>
			  <p style="text-align:justify; text-justify:inter-ideograph;"> For any information, please send an e-mail to <a href="mailto:slzhang.jdl@pku.edu.cn">Shiliang Zhang</a>, <a href="mailto:liguorong@ucas.edu.cn">Guorong Li</a> and <a href="mailto:wgzhang@hit.edu.cn">Weigang Zhang</a>.</p>
		  <br>
			  <a href="https://iapr.org" target="_blank"><img src="img/IAPR.gif" alt="iapr"></a>
				  &nbsp;&nbsp;
			 <a href="https://www.computer.org" target="_blank"><img src="img/IEEE.png" alt="ieee"></a>
      </div>
	</div>
</div>

</body>
</html>
